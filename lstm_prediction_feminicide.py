# -*- coding: utf-8 -*-
"""LSTM - prediction feminicide.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vUvhNAhBxJjl4QPZ3nfqzyZP5CsA9akR
"""

!pip install optuna

# Montar o Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Imports
from math import inf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import optuna

# Carregar dataset
dataset_path = '/content/drive/MyDrive/Brenda/Df_eng_final_combined_no_duplicates_v3.csv'
df = pd.read_csv(dataset_path)

# Pré-processamento da data
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df.dropna(subset=['Date'], inplace=True)

print(len(df))

# Engenharia de atributos temporais
df['day_of_week'] = df['Date'].dt.dayofweek
df['month'] = df['Date'].dt.month
df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

# Agrupar por data
df_grouped = df.groupby('Date').size().reset_index(name='report_count')
df_grouped = df_grouped.sort_values('Date')
df_grouped.set_index('Date', inplace=True)

"""### Checking Report Count by Year"""

test_df = df_grouped.copy()

test_df.index.day_of_week

years = []

for i in test_df.index:
  if i.year not in years:
    years.append(i.year)

report_counts_by_year = []
for year in years:
  report_counts_by_year.append(test_df[test_df.index.year == year]['report_count'])

report_counts_by_year[0].values[0]

count_reports_by_week = [0, 0, 0, 0, 0, 0, 0]
weeks_names = ['mon', 'tue', 'wed', 'thr', 'fri', 'sat', 'sun']

for i in range(len(report_counts_by_year)):
  for j in range(len(report_counts_by_year[i])):
    count_reports_by_week[report_counts_by_year[i].index.day_of_week[j]] += report_counts_by_year[i].values[j]

plt.bar(weeks_names, count_reports_by_week)
plt.title('Number of reports by week')
plt.xlabel('Days of the week')
plt.ylabel('Reports')
plt.show()

count_reports_by_months = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
months_names = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']

for i in range(len(report_counts_by_year)):
  for j in range(len(report_counts_by_year[i])):
    count_reports_by_months[report_counts_by_year[i].index.month[j]-1] += report_counts_by_year[i].values[j]

plt.bar(months_names, count_reports_by_months)
plt.title('Number of reports by Month')
plt.xlabel('Months')
plt.ylabel('Reports')
plt.show()

df_grouped.head()

# Visualizar os dados
plt.figure(figsize=(10,6))
plt.plot(df_grouped.index, df_grouped['report_count'], label='Número de denúncias por dia')
plt.xlabel('Data')
plt.ylabel('Número de denúncias')
plt.title('Série Temporal de Denúncias')
plt.legend()
plt.show()

"""### Detecting outliers"""

# 1. Calcular Q1, Q3 e IQR
Q1 = df_grouped['report_count'].quantile(0.25)
Q3 = df_grouped['report_count'].quantile(0.75)
IQR = Q3 - Q1

# 2. Definir os limites para detecção de outliers
limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

print(f"Limite inferior para outliers: {limite_inferior}")
print(f"Limite superior para outliers: {limite_superior}")

# 3. Identificar os outliers
outliers_iqr = df_grouped[(df_grouped['report_count'] < limite_inferior) | (df_grouped['report_count'] > limite_superior)]

print("\nOutliers encontrados pelo método IQR:")
print(outliers_iqr)

# 4. Visualizar os resultados
plt.figure(figsize=(18, 6))
plt.plot(df_grouped.index, df_grouped['report_count'], label='Contagem de Reports', color='blue')
plt.scatter(outliers_iqr.index, outliers_iqr['report_count'], color='red', s=50, label='Outliers (IQR)')
plt.title('Detecção de Outliers com Método IQR')
plt.legend()
plt.show()

"""### Capping Outliers"""

df_test = df_grouped.copy()

df_test.loc[df_test['report_count'] > limite_superior, 'report_count'] = limite_superior
df_test.loc[df_test['report_count'] < limite_inferior, 'report_count'] = limite_inferior

# 4. Visualizar os resultados
plt.figure(figsize=(18, 6))
plt.plot(df_test.index, df_test['report_count'], label='Reports', color='blue')
plt.title('Reports after the capping process')
plt.legend()
plt.show()

"""### Normalization"""

# Normalização
scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df_test)

print(np.isnan(df_scaled).any())

# Criar sequências
def create_sequences(data, look_back):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:i + look_back, 0])
        y.append(data[i + look_back, 0])
    return np.array(X), np.array(y)

"""### Using otimizer"""

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0)

def build_model(learning_rate, units, dropout_rate, look_back):
  optimizer = Adam(learning_rate=learning_rate)

  # Modelo Bidirectional LSTM com Dropout
  model = Sequential()

  # Camada 1 (escondida)
  model.add(Bidirectional(LSTM(units, return_sequences=True), input_shape=(look_back, 1))) # BiLSTM espera receber uma sequência completa (o "produto na esteira"), então necessita do return_sequences = True
  model.add(Dropout(dropout_rate))

  # Camada 2 (escondida)
  model.add(Bidirectional(LSTM(int(units/2), return_sequences=True)))
  model.add(Dropout(dropout_rate))

  # Camada 3 (escondida)
  model.add(Bidirectional(LSTM(int(units/4)))) # Uma camada Dense espera receber apenas um vetor (o "relatório final"), então a última camada BiLSTM necessita do return_sequences = False
  model.add(Dropout(dropout_rate))

  # Camada 4 (escondida)
  model.add(Dense(int(units/4), activation='relu'))
  model.add(Dropout(dropout_rate))

  # Camada 5 (escondida)
  model.add(Dense(int(units/8), activation='relu'))

  # Camada de saída
  model.add(Dense(1))
  model.compile(optimizer=optimizer, loss='mean_squared_error')

  return model

def objective(trial):
  look_back = trial.suggest_categorical("look_back", [7, 14, 30, 90, 180, 365, 1825])
  dropout_rate = trial.suggest_float("dropout_rate", 0.2, 0.5, step=0.1)
  learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-2, log=True)
  batch_size = trial.suggest_categorical("batch_size", [16, 32, 64, 128])
  epochs = trial.suggest_int("epochs", 100, 1000, step=100)
  units = trial.suggest_categorical("units", [32, 64, 128, 256])

  X, y = create_sequences(df_scaled, look_back)
  X = X.reshape((X.shape[0], X.shape[1], 1))

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

  model = build_model(learning_rate, units, dropout_rate, look_back)
  model.fit(
    X_train, y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=0.2,
    shuffle=False,
    callbacks=[early_stop, reduce_lr],
    verbose=False
  )

  y_pred = model.predict(X_test)

  # checking NaNs - exploding gradient
  print(f"Trial {trial.number}: y_pred contém NaNs? {np.isnan(y_pred).any()}")

  mae = mean_absolute_error(y_test, y_pred)
  rmse = np.sqrt(mean_squared_error(y_test, y_pred))
  mape = mean_absolute_percentage_error(y_test, y_pred)

  # regra de Pareto
  return mae, rmse, mape

study = optuna.create_study(directions=["minimize", "minimize", "minimize"])
study.optimize(objective, n_trials=30)

fig = optuna.visualization.plot_pareto_front(study,
                                             targets=lambda t: (t.values[0], t.values[1], t.values[2]),
                                             target_names=["mae", "rmse", "mape"])
fig.show()

best_index = len(study.best_trials) - 1
print(best_index)
print(study.best_trials[best_index].params)

best_look_back = study.best_trials[best_index].params["look_back"]
best_dropout_rate = study.best_trials[best_index].params["dropout_rate"]
best_learning_rate = study.best_trials[best_index].params["learning_rate"]
best_batch_size = study.best_trials[best_index].params["batch_size"]
best_epochs = study.best_trials[best_index].params["epochs"]
best_units = study.best_trials[best_index].params["units"]

def get_metrics(y_test, y_pred):
  # Avaliação das métricas no conjunto de teste
  mae = mean_absolute_error(y_test, y_pred)
  rmse = np.sqrt(mean_squared_error(y_test, y_pred))
  mape = mean_absolute_percentage_error(y_test, y_pred)
  return mae, rmse, mape

X, y = create_sequences(df_scaled, best_look_back)
X = X.reshape((X.shape[0], X.shape[1], 1))

X.shape

"""### Train with TimeSeriesSplit"""

fold_losses = []
MAE = []
RMSE = []
MAPE = []
predictions= []
real_values = []

tscv = TimeSeriesSplit(n_splits=3)

for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
  print(f"== Start Fold {fold + 1} ==")
  X_train, X_test = X[train_idx], X[test_idx]
  y_train, y_test = y[train_idx], y[test_idx]

  model = build_model(best_learning_rate, best_units, best_dropout_rate, best_look_back)
  print("Iniciando o treinamento inicial")
  history = model.fit(
    X_train, y_train,
    epochs=best_epochs,
    batch_size=best_batch_size,
    validation_split=0.2,
    shuffle=False,
    callbacks=[early_stop, reduce_lr],
    verbose=True
  )

  # Congelamento dos pesos - fine-tuning
  print("Iniciando o fine-tuning")
  model.layers[0].trainable = False
  # model.layers[2].trainable = False

  model.compile(optimizer=Adam(learning_rate=(best_learning_rate/10)), loss='mean_squared_error')

  loss = model.evaluate(X_test, y_test, verbose=0)
  fold_losses.append(loss)

  # Avaliação no conjunto
  y_pred = model.predict(X_test)
  y_pred_inv = scaler.inverse_transform(y_pred)
  y_inv = scaler.inverse_transform(y_test.reshape(-1, 1))
  print(f'Y_PRED shape: {y_pred_inv.shape}')
  print(f'Y: {y_inv.shape}')

  predictions.append(y_pred_inv)
  real_values.append(y_inv)

  mae_train, rmse_train, mape_train = get_metrics(y_inv, y_pred_inv)
  MAE.append(mae_train)
  RMSE.append(rmse_train)
  MAPE.append(mape_train)

  print(f"== End Fold {fold + 1} ==")

"""### Visualization of the results"""

for i in range(len(MAE)):
  print("\n--- Avaliação no Treino ---")
  print("MAE:", MAE[i])
  print("RMSE:", RMSE[i])
  print("MAPE:", MAPE[i])

# Visualização dos resultados
plt.figure(figsize=(10, 6))
plt.plot(real_values[0], label="Real")
plt.plot(predictions[0], label="Previsto")
plt.title("Previsão vs Real")
plt.legend()
plt.show()

# Visualização dos resultados
plt.figure(figsize=(10, 6))
plt.plot(real_values[1], label="Real")
plt.plot(predictions[1], label="Previsto")
plt.title("Previsão vs Real")
plt.legend()
plt.show()

# Visualização dos resultados
plt.figure(figsize=(10, 6))
plt.plot(real_values[2], label="Real")
plt.plot(predictions[2], label="Previsto")
plt.title("Previsão vs Real")
plt.legend()
plt.show()

# Visualização dos resíduos (erros)
residuos_treino = real_values_train[0].flatten() - predictions_train[0].flatten()
residuos_teste = real_values_test[0].flatten() - predictions_test[0].flatten()

plt.figure(figsize=(10, 4))
plt.plot(residuos_treino, label='Resíduos - Treino', color='orange')
plt.axhline(0, color='red', linestyle='--')
plt.title("Resíduos do Modelo no Conjunto de Treino")
plt.legend()
plt.show()

plt.figure(figsize=(10, 4))
plt.plot(residuos_teste, label='Resíduos - Teste')
plt.axhline(0, color='red', linestyle='--')
plt.title("Resíduos do Modelo no Conjunto de Teste")
plt.legend()
plt.show()

# Visualização dos resíduos (erros)
residuos_treino = real_values_train[1].flatten() - predictions_train[1].flatten()
residuos_teste = real_values_test[1].flatten() - predictions_test[1].flatten()

plt.figure(figsize=(10, 4))
plt.plot(residuos_treino, label='Resíduos - Treino', color='orange')
plt.axhline(0, color='red', linestyle='--')
plt.title("Resíduos do Modelo no Conjunto de Treino")
plt.legend()
plt.show()

plt.figure(figsize=(10, 4))
plt.plot(residuos_teste, label='Resíduos - Teste')
plt.axhline(0, color='red', linestyle='--')
plt.title("Resíduos do Modelo no Conjunto de Teste")
plt.legend()
plt.show()

# Visualização dos resíduos (erros)
residuos_treino = real_values_train[2].flatten() - predictions_train[2].flatten()
residuos_teste = real_values_test[2].flatten() - predictions_test[2].flatten()

plt.figure(figsize=(10, 4))
plt.plot(residuos_treino, label='Resíduos - Treino', color='orange')
plt.axhline(0, color='red', linestyle='--')
plt.title("Resíduos do Modelo no Conjunto de Treino")
plt.legend()
plt.show()

plt.figure(figsize=(10, 4))
plt.plot(residuos_teste, label='Resíduos - Teste')
plt.axhline(0, color='red', linestyle='--')
plt.title("Resíduos do Modelo no Conjunto de Teste")
plt.legend()
plt.show()